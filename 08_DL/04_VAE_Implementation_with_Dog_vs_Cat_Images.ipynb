{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUSxMpZNwMfzo8nU7FrvOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usshaa/BK_BIRLA_DL/blob/main/08_DL/04_VAE_Implementation_with_Dog_vs_Cat_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE Implementation with Kaggle Dog vs Cat Images"
      ],
      "metadata": {
        "id": "qw83jUm45Q2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o dog-and-cat-classification-dataset.zip \\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/bhavikjikadara/dog-and-cat-classification-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQhYePvW5Zrt",
        "outputId": "deaa216c-60cf-45a5-e1a4-7d42c8f2200e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  775M  100  775M    0     0  69.7M      0  0:00:11  0:00:11 --:--:--  125M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q dog-and-cat-classification-dataset.zip -d ./data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM3pgR1o5b6g",
        "outputId": "5da4be2b-b280-4a06-9837-eb1d61240312"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace ./data/PetImages/Cat/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace ./data/PetImages/Cat/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import os"
      ],
      "metadata": {
        "id": "bdd8Lq515TRY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 64\n",
        "DATA_DIR = \"./data/PetImages\""
      ],
      "metadata": {
        "id": "rhHJuN1z6HXi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LATENT_DIM = 64   # Latent vector size\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
      ],
      "metadata": {
        "id": "ncIrD8ZT5VMX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cats = []\n",
        "dogs = []"
      ],
      "metadata": {
        "id": "Au9FComZ6SJH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img in os.listdir(os.path.join(DATA_DIR, \"Cat\"))[:5000]:  # limit for speed\n",
        "    try:\n",
        "        img_array = cv2.imread(os.path.join(DATA_DIR, \"Cat\", img))\n",
        "        img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "        cats.append(img_array)\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "HzV4FR4-6UZx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img in os.listdir(os.path.join(DATA_DIR, \"Dog\"))[:5000]:\n",
        "    try:\n",
        "        img_array = cv2.imread(os.path.join(DATA_DIR, \"Dog\", img))\n",
        "        img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "        dogs.append(img_array)\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "9TLAC4Ca6V3V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(cats + dogs) / 255.0  # Normalize to [0,1]\n",
        "print(\"Dataset shape:\", X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2OloG1_6aIZ",
        "outputId": "cf2ceb9f-9b4b-47a5-ca47-d8867ef70862"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (10000, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "S_qvibdq6bsK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling layer for reparameterization trick\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "DfQrJtKu5t4B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "def build_encoder():\n",
        "    encoder_inputs = layers.Input(shape=IMG_SHAPE)\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "    return models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")"
      ],
      "metadata": {
        "id": "hMCkA8cQ5zMn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "def build_decoder():\n",
        "    latent_inputs = layers.Input(shape=(LATENT_DIM,))\n",
        "    x = layers.Dense(16 * 16 * 64, activation=\"relu\")(latent_inputs)  # adjust for IMG_SIZE=64\n",
        "    x = layers.Reshape((16, 16, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "    decoder_outputs = layers.Conv2DTranspose(3, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "    return models.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "metadata": {
        "id": "qh6-Q_G958dL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE class\n",
        "class VAE(tf.keras.models.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.mse(data, reconstruction), axis=(1, 2)))\n",
        "            kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        return {\"loss\": total_loss, \"reconstruction_loss\": reconstruction_loss, \"kl_loss\": kl_loss}"
      ],
      "metadata": {
        "id": "JrVwnnTf5_Xt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build VAE\n",
        "encoder = build_encoder()\n",
        "decoder = build_decoder()\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=tf.keras.optimizers.Adam(), run_eagerly=True)"
      ],
      "metadata": {
        "id": "FTA7qkLV_TUm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train VAE\n",
        "vae.fit(X_train, epochs=15, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_H8D9LI6MDe",
        "outputId": "1089686d-ee13-4ddd-9c79-e04c3d5a7e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - kl_loss: 1.6023 - loss: 268.9652 - reconstruction_loss: 267.3629\n",
            "Epoch 2/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 2s/step - kl_loss: 4.0613 - loss: 243.1698 - reconstruction_loss: 239.1086\n",
            "Epoch 3/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - kl_loss: 6.7463 - loss: 223.9952 - reconstruction_loss: 217.2489\n",
            "Epoch 4/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - kl_loss: 8.2493 - loss: 217.7059 - reconstruction_loss: 209.4566\n",
            "Epoch 5/15\n",
            "\u001b[1m58/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 2s/step - kl_loss: 8.1451 - loss: 215.7073 - reconstruction_loss: 207.5623"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct images\n",
        "z_mean, z_log_var, z = encoder.predict(X_test[:10])\n",
        "reconstructed = decoder.predict(z)"
      ],
      "metadata": {
        "id": "4DN4Klt_6hSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot comparison\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(X_test[i])\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # VAE Reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(reconstructed[i])\n",
        "    plt.title(\"VAE Recon\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "orgJYipK6j05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Difference\n",
        "\n",
        "* **Autoencoder (AE)**: Learns **direct mapping** → Compress → Reconstruct.\n",
        "* **Variational Autoencoder (VAE)**: Learns a **distribution in latent space** → Can generate new samples, not just reconstruct."
      ],
      "metadata": {
        "id": "xbkszX156n7U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLXIuPN55H0Y"
      },
      "outputs": [],
      "source": []
    }
  ]
}